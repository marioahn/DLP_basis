{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14장 모델 성능 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 와인데이터\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "# 학습셋, 테스트셋 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# 모델이 저장되는 조건 설정\n",
    "modelpath = \"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, \n",
    "                    verbose=1, callbacks=[early_stopping_callback, checkpointer]) \n",
    "\n",
    "# 테스트 결과 출력\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df\n",
    "\n",
    "# val_loss, loss값들 저장\n",
    "y_vloss = hist_df['val_loss']\n",
    "y_loss = hist_df['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c='blue', markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15장 실제 데이터로 만들어 보는모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)주택가격 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/house_train.csv\")\n",
    "df\n",
    "\n",
    "# 각 데이터가 어떤 유형으로 되어 있는지? - 정수,실수,오브젝트형 등등 다양함\n",
    "df.dtypes\n",
    "\n",
    "# 판다스의 get_dummies()를 이용해서 카테고리형 변수를 0, 1로 이루어진 변수로 change (원핫 인코딩)\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# 결측치 대체\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# 데이터 사이의 상관관계 저장\n",
    "df_corr = df.corr()\n",
    "\n",
    "# *집값과 관련이 큰 것부터 순서대로 저장\n",
    "df_corr_sort = df_corr.sort_values('SalePrice', ascending=False)\n",
    "\n",
    "# 집값을 제외한 나머지 열을 저장한다\n",
    "cols_train = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF']\n",
    "X_train_pre = df[cols_train]\n",
    "\n",
    "# 집값을 저장\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "# 학습:테스트=8:2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_pre, y, test_size=0.2)\n",
    "\n",
    "# 모델 구조 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# 모델을 실행\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 20번 이상 결과가 향상되지 않으면 자동으로 중단\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# 모델의 이름 설정\n",
    "modelpath = './data/model/Ch15-house.keras'\n",
    "\n",
    "# 최적화된 모델 업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss',\n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "# 실행 관련 설정을 하는 부분 - 전체의 20%를 검증셋으로 설정\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=2000,\n",
    "                    batch_size=32, callbacks=[early_stopping_callback, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주택 가격 예측 모델\n",
    "## 학습 결과를 시각화하기 위해 예측 값과 실제 값, 실행 번호가 들어갈 빈 리스트를 만들고,\n",
    "## 25개의 샘플로부터 얻은 결과를 채워 넣는다\n",
    "real_prices = []\n",
    "pred_prices = [] # 예상가\n",
    "X_num = []\n",
    "\n",
    "n_iter = 0\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "\n",
    "for i in range(25):\n",
    "    real = y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print('실제가격: {:.2f}, 예상가격: {:.2f}'.format(real, prediction))\n",
    "    real_prices.append(real)\n",
    "\n",
    "    pred_prices.append(prediction)\n",
    "    n_iter += 1\n",
    "    X_num.append(n_iter)\n",
    "\n",
    "# 이제 위 결과를 그래프를 통해 비교해보자\n",
    "plt.plot(X_num, pred_prices, label='predicted price')\n",
    "plt.plot(X_num, real_prices, label='real price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)추가데이터 - car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='train')\n",
    "test_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16장 이미지 인식의 꽃, 컨볼루션신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)mnist데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋을 불러와 학습&테스트셋으로 저장\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 학습셋과 테스트셋이 각각 몇 개의 이미지로 되어 있는지 화깅ㄴ\n",
    "print('학습셋 이미지 수: %d개' % (X_train.shape[0]))\n",
    "print('테스트 이미지 수: %d개' % (X_test.shape[0]))\n",
    "\n",
    "# 첫 번째 이미지를 확인\n",
    "plt.imshow(X_train[0], cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# 이미지가 인식되는 원리\n",
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%-3s' % i)\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "# 차원 변환 과정을 연습해보자\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255\n",
    "\n",
    "# 클래스값을 확인\n",
    "print('class: %d' % (y_train[0]))\n",
    "\n",
    "# 바이너리화 과정을 연습해보자\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 기본 프레임\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # 이건 10개 output - 0~9까지 [0,0,0,0,0,1,0,0,0,0]\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 최적화함수는 adam\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/MNIST_MLP.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장 - 그래프로 보여줄거임\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2)최종 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16장 최종 코드\n",
    "\n",
    "# 데이터 load & 전처리\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# *컨볼루션 신경망 설정\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) # 10개 output\n",
    "\n",
    "# 모델 실행(compile) 옵션 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/MNIST_CNN.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행ㄱㄱ\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장 - 그래프로 보여줄거임\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)추가 데이터 - 7분 30초걸림 후.. (4번째 줄 빼고, 동일함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# fashion_mnist데이터셋을 불러와 학습&테스트셋으로 저장\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# *컨볼루션 신경망 설정\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "\n",
    "# 모델 실행(compile) 옵션 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/fashion_MNIST_CNN.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19장 - 세상에 없는 얼굴 GAN, 오토인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)생성자 - 모델이름을 generator로 정한다\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2))) # relu에서 손실되는 값 조금이라도 더 보완한 함수\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)판별자 - 모델이름을 discriminator로 정한다\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding='same'))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid')) # sigmoid를 왜 쓰냐면, 0과 1을 판별하기 때문이지!\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "discriminator.trainable = False # 학습할 것 아니니까!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_train(epoch, batch_size, saving_interval): # 세 가지 변수 지정\n",
    "    # MNIST 데이터load. 단, 테스트는 필요없고 이미지만 사용\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # 가로28픽셀, 세로28픽셀이고 흑백이므로 1을 설정\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "    # 0~255사이 픽셀값에서 127.5를 뺀 후, 127.5로 나누면 -1~1사이 값으로 바뀐다\n",
    "    X_train = (X_train-127.5)/127.5\n",
    "\n",
    "    true = np.ones((batch_size, 1)) # 1\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size) # 2\n",
    "    imgs = X_train[idx] # 3\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, true) # 4\n",
    "\n",
    "    fake = np.zeros((batch_size, 1)) # 1\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100)) # 2\n",
    "    gen_imgs = generator.predict(noise) # 3\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) # 4\n",
    "\n",
    "    # d_loss_real, d_loss_fake의 평균이 바로 판.별.자의 오차\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 예제 파일에서 data 폴더 아래에 이미지가 저장될 gan_images 폴더가 함께 제공됩니다.\n",
    "# 만약 이미지가 저장될 폴더가 없다면 아래 코드의 주석을 해제해 gan_images 폴더를 만듭니다.\n",
    "# import os\n",
    "# if not os.path.exists(\"./data/gan_images\"):\n",
    "#    os.makedirs(\"./data/gan_images\")\n",
    "\n",
    "# 생성자 모델을 만듭니다.\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))\n",
    "\n",
    "# 판별자 모델을 만듭니다.\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator.trainable = False\n",
    "\n",
    "# 생성자와 판별자 모델을 연결시키는 gan 모델을 만듭니다.\n",
    "ginput = Input(shape=(100,))\n",
    "dis_output = discriminator(generator(ginput))\n",
    "gan = Model(ginput, dis_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()\n",
    "\n",
    "# 신경망을 실행시키는 함수를 만듭니다.\n",
    "def gan_train(epoch, batch_size, saving_interval):\n",
    "\n",
    "  # MNIST 데이터를 불러옵니다.\n",
    "  (X_train, _), (_, _) = mnist.load_data()\n",
    "  X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "  X_train = (X_train - 127.5) / 127.5  # 픽셀 값은 0에서 255 사이의 값 - 이전에 255로 나누어 줄때는 이를 0~1 사이의 값으로 바꾸었던 것인데, 여기서는 127.5를 빼준 뒤 127.5로 나누어 줌으로 인해 -1에서 1사이의 값으로 바뀌게 된다\n",
    "\n",
    "  true = np.ones((batch_size, 1))\n",
    "  fake = np.zeros((batch_size, 1))\n",
    "\n",
    "  for i in range(epoch):\n",
    "    # 실제 데이터를 판별자에 입력하는 부분\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, true)\n",
    "\n",
    "    # 가상 이미지를 판별자에 입력하는 부분\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "    # 판별자와 생성자의 오차를 계산\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    g_loss = gan.train_on_batch(noise, true)\n",
    "\n",
    "    print(\"여기!!!\", d_loss)\n",
    "\n",
    "    print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss)\n",
    "\n",
    "    # 이 부분은 중간 과정을 이미지로 저장해 주는 부분이다. 본 장의 주요 내용과 관련이 없어 소스 코드만 첨부\n",
    "    # 만들어진 이미지들은 gan_images 폴더에 저장된다\n",
    "    if i % saving_interval == 0:\n",
    "        #r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (25, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        count = 0\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
    "                axs[j, k].axis('off')\n",
    "                count += 1\n",
    "        fig.savefig(\"./data/gan_images/gan_mnist_%d.png\" % i)\n",
    "\n",
    "\n",
    "gan_train(2001, 32, 200) # 2000번 반복(+1)되고, 배치 크기는 32, 200번마다 결과가 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
