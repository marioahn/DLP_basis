{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14장 모델 성능 향상시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 와인데이터\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "# 학습셋, 테스트셋 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# 모델이 저장되는 조건 설정\n",
    "modelpath = \"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, \n",
    "                    verbose=1, callbacks=[early_stopping_callback, checkpointer]) \n",
    "\n",
    "# 테스트 결과 출력\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df\n",
    "\n",
    "# val_loss, loss값들 저장\n",
    "y_vloss = hist_df['val_loss']\n",
    "y_loss = hist_df['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c='red', markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c='blue', markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15장 실제 데이터로 만들어 보는모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)주택가격 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/house_train.csv\")\n",
    "df\n",
    "\n",
    "# 각 데이터가 어떤 유형으로 되어 있는지? - 정수,실수,오브젝트형 등등 다양함\n",
    "df.dtypes\n",
    "\n",
    "# 판다스의 get_dummies()를 이용해서 카테고리형 변수를 0, 1로 이루어진 변수로 change (원핫 인코딩)\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# 결측치 대체\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# 데이터 사이의 상관관계 저장\n",
    "df_corr = df.corr()\n",
    "\n",
    "# *집값과 관련이 큰 것부터 순서대로 저장\n",
    "df_corr_sort = df_corr.sort_values('SalePrice', ascending=False)\n",
    "\n",
    "# 집값을 제외한 나머지 열을 저장한다\n",
    "cols_train = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF']\n",
    "X_train_pre = df[cols_train]\n",
    "\n",
    "# 집값을 저장\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "# 학습:테스트=8:2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_pre, y, test_size=0.2)\n",
    "\n",
    "# 모델 구조 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# 모델을 실행\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 20번 이상 결과가 향상되지 않으면 자동으로 중단\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# 모델의 이름 설정\n",
    "modelpath = './data/model/Ch15-house.keras'\n",
    "\n",
    "# 최적화된 모델 업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss',\n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "# 실행 관련 설정을 하는 부분 - 전체의 20%를 검증셋으로 설정\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=2000,\n",
    "                    batch_size=32, callbacks=[early_stopping_callback, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주택 가격 예측 모델\n",
    "## 학습 결과를 시각화하기 위해 예측 값과 실제 값, 실행 번호가 들어갈 빈 리스트를 만들고,\n",
    "## 25개의 샘플로부터 얻은 결과를 채워 넣는다\n",
    "real_prices = []\n",
    "pred_prices = [] # 예상가\n",
    "X_num = []\n",
    "\n",
    "n_iter = 0\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "\n",
    "for i in range(25):\n",
    "    real = y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print('실제가격: {:.2f}, 예상가격: {:.2f}'.format(real, prediction))\n",
    "    real_prices.append(real)\n",
    "\n",
    "    pred_prices.append(prediction)\n",
    "    n_iter += 1\n",
    "    X_num.append(n_iter)\n",
    "\n",
    "# 이제 위 결과를 그래프를 통해 비교해보자\n",
    "plt.plot(X_num, pred_prices, label='predicted price')\n",
    "plt.plot(X_num, real_prices, label='real price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)추가데이터 - car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='train')\n",
    "test_df = pd.read_excel('https://github.com/cranberryai/todak_todak_python/blob/master/machine_learning/regression/carprice_E1SUl6b.xlsx?raw=true', sheet_name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16장 이미지 인식의 꽃, 컨볼루션신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)mnist데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋을 불러와 학습&테스트셋으로 저장\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 학습셋과 테스트셋이 각각 몇 개의 이미지로 되어 있는지 화깅ㄴ\n",
    "print('학습셋 이미지 수: %d개' % (X_train.shape[0]))\n",
    "print('테스트 이미지 수: %d개' % (X_test.shape[0]))\n",
    "\n",
    "# 첫 번째 이미지를 확인\n",
    "plt.imshow(X_train[0], cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "# 이미지가 인식되는 원리\n",
    "for x in X_train[0]:\n",
    "    for i in x:\n",
    "        sys.stdout.write('%-3s' % i)\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "# 차원 변환 과정을 연습해보자\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = X_train/255\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255\n",
    "\n",
    "# 클래스값을 확인\n",
    "print('class: %d' % (y_train[0]))\n",
    "\n",
    "# 바이너리화 과정을 연습해보자\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 기본 프레임\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # 이건 10개 output - 0~9까지 [0,0,0,0,0,1,0,0,0,0]\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 최적화함수는 adam\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/MNIST_MLP.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장 - 그래프로 보여줄거임\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2)최종 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16장 최종 코드\n",
    "\n",
    "# 데이터 load & 전처리\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# *컨볼루션 신경망 설정\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) # 10개 output\n",
    "\n",
    "# 모델 실행(compile) 옵션 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/MNIST_CNN.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행ㄱㄱ\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장 - 그래프로 보여줄거임\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)추가 데이터 - 7분 30초걸림 후.. (4번째 줄 빼고, 동일함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# fashion_mnist데이터셋을 불러와 학습&테스트셋으로 저장\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# *컨볼루션 신경망 설정\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "\n",
    "# 모델 실행(compile) 옵션 설정\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 최적화를 위한 설정 구간\n",
    "modelpath = './data/fashion_MNIST_CNN.keras'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 실행\n",
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=30, batch_size=200,\n",
    "                    verbose=0, callbacks=[early_stopping_callback, checkpointer])\n",
    "\n",
    "# 테스트 정확도를 출력\n",
    "print('\\n Test Accuracy: %.4f' % (model.evaluate(X_test, y_test)[1]))\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19장 - 세상에 없는 얼굴 GAN, 오토인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)생성자 - 모델이름을 generator로 정한다\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2))) # relu에서 손실되는 값 조금이라도 더 보완한 함수\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)판별자 - 모델이름을 discriminator로 정한다\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding='same'))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid')) # sigmoid를 왜 쓰냐면, 0과 1을 판별하기 때문이지!\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "discriminator.trainable = False # 학습할 것 아니니까!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
